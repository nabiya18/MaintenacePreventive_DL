{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCYV7Zbi9o/KKsJIfHLUC6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nabiya18/MaintenacePreventive_DL/blob/main/Maintenancepredictive.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualisation des éléments dans notre dataset"
      ],
      "metadata": {
        "id": "8N60xasqbBIU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Charger le dataset\n",
        "data = pd.read_csv('dataset_pannes.csv')  # Remplacez ce chemin par l'emplacement réel de votre fichier\n",
        "\n",
        "# 1. Afficher les premières lignes du dataset\n",
        "print(\"Aperçu des premières lignes du dataset :\")\n",
        "print(data.head())  # Affiche les 5 premières lignes par défaut\n",
        "\n",
        "# 2. Obtenir des informations générales sur le dataset (types de données, valeurs manquantes, etc.)\n",
        "print(\"\\nInformations générales sur les données :\")\n",
        "print(data.info())  # Affiche les informations sur les colonnes, types et valeurs manquantes\n",
        "\n",
        "# 3. Statistiques descriptives du dataset pour mieux comprendre les données numériques\n",
        "print(\"\\nStatistiques descriptives du dataset :\")\n",
        "print(data.describe())  # Affiche des statistiques comme la moyenne, écart-type, min, max pour les colonnes numériques\n",
        "\n",
        "# 4. Vérifier s'il y a des valeurs manquantes\n",
        "print(\"\\nVérifier les valeurs manquantes dans le dataset :\")\n",
        "print(data.isnull().sum())  # Compte le nombre de valeurs manquantes par colonne\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lSIPQ7RQabYK",
        "outputId": "1e99185c-14e6-430e-a312-baa6eb605171"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aperçu des premières lignes du dataset :\n",
            "                  date  equipement_id  time  temperature  vibration   courant  \\\n",
            "0  2024-01-01 00:00:00              0     0    52.483571   6.399355  8.649643   \n",
            "1  2024-01-01 00:01:00              0     1    49.318678   5.925634  9.715963   \n",
            "2  2024-01-01 00:02:00              0     2    53.258443   5.061630  8.425160   \n",
            "3  2024-01-01 00:03:00              0     3    57.645149   4.356063  9.399077   \n",
            "4  2024-01-01 00:04:00              0     4    48.869233   5.702223  6.232771   \n",
            "\n",
            "   alerts  panne  energy_consumption  cpu_usage  memory_usage  \n",
            "0       0      0          400.971402  76.538337     60.354634  \n",
            "1       0      0          447.250718  37.634755     53.877272  \n",
            "2       0      0          470.648580  66.429584     39.491884  \n",
            "3       1      0          507.483446  14.048207     94.795319  \n",
            "4       0      0          551.208116  47.121475     83.626818  \n",
            "\n",
            "Informations générales sur les données :\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10000 entries, 0 to 9999\n",
            "Data columns (total 11 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   date                10000 non-null  object \n",
            " 1   equipement_id       10000 non-null  int64  \n",
            " 2   time                10000 non-null  int64  \n",
            " 3   temperature         10000 non-null  float64\n",
            " 4   vibration           10000 non-null  float64\n",
            " 5   courant             10000 non-null  float64\n",
            " 6   alerts              10000 non-null  int64  \n",
            " 7   panne               10000 non-null  int64  \n",
            " 8   energy_consumption  10000 non-null  float64\n",
            " 9   cpu_usage           10000 non-null  float64\n",
            " 10  memory_usage        10000 non-null  float64\n",
            "dtypes: float64(6), int64(4), object(1)\n",
            "memory usage: 859.5+ KB\n",
            "None\n",
            "\n",
            "Statistiques descriptives du dataset :\n",
            "       equipement_id          time   temperature     vibration       courant  \\\n",
            "count   10000.000000  10000.000000  10000.000000  10000.000000  10000.000000   \n",
            "mean        4.500000    499.500000     54.973672      5.499861     12.503180   \n",
            "std         2.872425    288.689425      5.744923      1.043286      2.465462   \n",
            "min         0.000000      0.000000     33.443999      1.817625      3.326689   \n",
            "25%         2.000000    249.750000     51.073540      4.789528     10.772296   \n",
            "50%         4.500000    499.500000     54.963166      5.498127     12.528844   \n",
            "75%         7.000000    749.250000     58.823145      6.213218     14.217510   \n",
            "max         9.000000    999.000000     80.825421      9.243625     22.327475   \n",
            "\n",
            "             alerts         panne  energy_consumption     cpu_usage  \\\n",
            "count  10000.000000  10000.000000        10000.000000  10000.000000   \n",
            "mean       0.749100      0.302400          499.621426     50.149226   \n",
            "std        0.433553      0.459321           50.224354     22.975595   \n",
            "min        0.000000      0.000000          276.719807     10.000675   \n",
            "25%        0.000000      0.000000          465.269357     30.691646   \n",
            "50%        1.000000      0.000000          499.480888     50.185211   \n",
            "75%        1.000000      1.000000          533.951523     70.082578   \n",
            "max        1.000000      1.000000          686.391667     89.995176   \n",
            "\n",
            "       memory_usage  \n",
            "count  10000.000000  \n",
            "mean      57.592323  \n",
            "std       21.637284  \n",
            "min       20.020259  \n",
            "25%       38.869312  \n",
            "50%       57.836389  \n",
            "75%       76.460031  \n",
            "max       94.987349  \n",
            "\n",
            "Vérifier les valeurs manquantes dans le dataset :\n",
            "date                  0\n",
            "equipement_id         0\n",
            "time                  0\n",
            "temperature           0\n",
            "vibration             0\n",
            "courant               0\n",
            "alerts                0\n",
            "panne                 0\n",
            "energy_consumption    0\n",
            "cpu_usage             0\n",
            "memory_usage          0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prétraitement des éléments de notre dataset"
      ],
      "metadata": {
        "id": "OPBax_9ybKBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
        "\n",
        "# Charger les données\n",
        "df = pd.read_csv(\"dataset_pannes.csv\", parse_dates=[\"date\"])\n",
        "df.sort_values(by=[\"equipement_id\", \"date\"], inplace=True)\n",
        "\n",
        "# Sélection des caractéristiques et normalisation\n",
        "features = [\"temperature\", \"vibration\", \"courant\", \"energy_consumption\", \"cpu_usage\", \"memory_usage\"]\n",
        "scaler = MinMaxScaler()\n",
        "df[features] = scaler.fit_transform(df[features])\n",
        "\n",
        "# Création de séquences temporelles\n",
        "sequence_length = 20  # Longueur des séquences\n",
        "X, y = [], []\n",
        "\n",
        "def create_sequences(data, labels, seq_length):\n",
        "    for i in range(len(data) - seq_length):\n",
        "        X.append(data[i : i + seq_length])\n",
        "        y.append(labels[i + seq_length])\n",
        "\n",
        "for eq_id in df[\"equipement_id\"].unique():\n",
        "    eq_data = df[df[\"equipement_id\"] == eq_id]\n",
        "    create_sequences(eq_data[features].values, eq_data[\"panne\"].values, sequence_length)\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Sauvegarde des données prétraitées\n",
        "np.save(\"X_sequences.npy\", X)\n",
        "np.save(\"y_labels.npy\", y)\n"
      ],
      "metadata": {
        "id": "Q2z-gC4cXyYr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Création de notre modèle de Deep learning"
      ],
      "metadata": {
        "id": "RLxbwjoLbQfk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "\n",
        "# Charger les données\n",
        "X = np.load(\"X_sequences.npy\")\n",
        "y = np.load(\"y_labels.npy\")\n",
        "\n",
        "\n",
        "# Définition du modèle amélioré\n",
        "model = Sequential([\n",
        "    LSTM(100, return_sequences=True, input_shape=(X.shape[1], X.shape[2])),\n",
        "    Dropout(0.3),\n",
        "    LSTM(100, return_sequences=True),\n",
        "    Dropout(0.3),\n",
        "    LSTM(50, return_sequences=False),\n",
        "    Dropout(0.2),\n",
        "    Dense(25, activation='relu'),  # Couche supplémentaire\n",
        "    Dense(10, activation='relu'),  # Ajout d'une nouvelle couche Dense\n",
        "    Dense(1, activation='sigmoid')  # Sortie binaire\n",
        "])\n",
        "\n",
        "# Compilation du modèle\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Sauvegarde du modèle avant entraînement\n",
        "model.save(\"modele_lstm.keras\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTT_xl_JaduG",
        "outputId": "6677eecc-31ac-4e20-a3b6-9330f78cff24"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entraînement de notre modèle"
      ],
      "metadata": {
        "id": "KwTclRrzbVPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# Activer l'exécution dynamique\n",
        "tf.config.run_functions_eagerly(True)\n",
        "\n",
        "# Charger les données\n",
        "X = np.load(\"X_sequences.npy\")\n",
        "y = np.load(\"y_labels.npy\")\n",
        "\n",
        "# Charger le modèle\n",
        "model = load_model(\"modele_lstm.keras\")\n",
        "\n",
        "# Définir un callback pour l'arrêt anticipé\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Compilation du modèle (nécessaire après le chargement)\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Entraînement du modèle\n",
        "history = model.fit(\n",
        "    X, y, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping]\n",
        ")\n",
        "\n",
        "# Sauvegarde du modèle entraîné\n",
        "model.save(\"modele_lstm_trained.keras\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hsJ_jpd6aruQ",
        "outputId": "4eab156a-cd5f-48c5-f4b8-9ae29fd84458"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/saving/saving_lib.py:757: UserWarning: Skipping variable loading for optimizer 'adam', because it has 32 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n",
            "/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m149s\u001b[0m 607ms/step - accuracy: 0.7043 - loss: 0.5941 - val_accuracy: 0.6990 - val_loss: 0.5379\n",
            "Epoch 2/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m193s\u001b[0m 571ms/step - accuracy: 0.7339 - loss: 0.5225 - val_accuracy: 0.7357 - val_loss: 0.5448\n",
            "Epoch 3/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 574ms/step - accuracy: 0.7326 - loss: 0.5255 - val_accuracy: 0.7378 - val_loss: 0.5109\n",
            "Epoch 4/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 581ms/step - accuracy: 0.7294 - loss: 0.5153 - val_accuracy: 0.7250 - val_loss: 0.5271\n",
            "Epoch 5/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 584ms/step - accuracy: 0.7426 - loss: 0.5160 - val_accuracy: 0.7423 - val_loss: 0.5346\n",
            "Epoch 6/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 582ms/step - accuracy: 0.7270 - loss: 0.5234 - val_accuracy: 0.7388 - val_loss: 0.5009\n",
            "Epoch 7/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 599ms/step - accuracy: 0.7432 - loss: 0.5134 - val_accuracy: 0.7398 - val_loss: 0.5023\n",
            "Epoch 8/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m141s\u001b[0m 575ms/step - accuracy: 0.7361 - loss: 0.5144 - val_accuracy: 0.7423 - val_loss: 0.4983\n",
            "Epoch 9/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 584ms/step - accuracy: 0.7291 - loss: 0.5239 - val_accuracy: 0.7393 - val_loss: 0.5031\n",
            "Epoch 10/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 578ms/step - accuracy: 0.7271 - loss: 0.5271 - val_accuracy: 0.7418 - val_loss: 0.5005\n",
            "Epoch 11/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 588ms/step - accuracy: 0.7379 - loss: 0.5142 - val_accuracy: 0.7087 - val_loss: 0.5172\n",
            "Epoch 12/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 572ms/step - accuracy: 0.7317 - loss: 0.5195 - val_accuracy: 0.7398 - val_loss: 0.4980\n",
            "Epoch 13/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m147s\u001b[0m 593ms/step - accuracy: 0.7286 - loss: 0.5236 - val_accuracy: 0.7398 - val_loss: 0.4946\n",
            "Epoch 14/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m202s\u001b[0m 592ms/step - accuracy: 0.7275 - loss: 0.5242 - val_accuracy: 0.7434 - val_loss: 0.4931\n",
            "Epoch 15/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m204s\u001b[0m 602ms/step - accuracy: 0.7370 - loss: 0.5194 - val_accuracy: 0.7480 - val_loss: 0.4967\n",
            "Epoch 16/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m201s\u001b[0m 597ms/step - accuracy: 0.7329 - loss: 0.5171 - val_accuracy: 0.7378 - val_loss: 0.5028\n",
            "Epoch 17/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m195s\u001b[0m 571ms/step - accuracy: 0.7390 - loss: 0.5176 - val_accuracy: 0.7469 - val_loss: 0.4964\n",
            "Epoch 18/50\n",
            "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 509ms/step - accuracy: 0.7350 - loss: 0.5176"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation de la performance du modèle"
      ],
      "metadata": {
        "id": "QT3O4VjRbgaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Charger les données\n",
        "X = np.load(\"X_sequences.npy\")\n",
        "y = np.load(\"y_labels.npy\")\n",
        "\n",
        "# Charger le modèle entraîné\n",
        "model = load_model(\"modele_lstm_trained.keras\")\n",
        "\n",
        "# Prédictions\n",
        "predictions = model.predict(X)\n",
        "predictions = (predictions > 0.45).astype(int)  # Convertir en 0 ou 1\n",
        "\n",
        "# Évaluation du modèle\n",
        "accuracy = accuracy_score(y, predictions)\n",
        "conf_matrix = confusion_matrix(y, predictions)\n",
        "class_report = classification_report(y, predictions)\n",
        "\n",
        "# Affichage des résultats\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
        "print(\"Classification Report:\\n\", class_report)\n",
        "\n",
        "# Visualisation de la matrice de confusion\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Pas de panne\", \"En panne\"], yticklabels=[\"Pas de panne\", \"En panne\"])\n",
        "plt.xlabel(\"Prediction\")\n",
        "plt.ylabel(\"Actuel\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()\n",
        "\n",
        "# Visualisation des performances par classe\n",
        "report_dict = classification_report(y, predictions, output_dict=True)\n",
        "classes = list(report_dict.keys())[:-3]\n",
        "f1_scores = [report_dict[cls]['f1-score'] for cls in classes]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x=classes, y=f1_scores, palette=\"viridis\")\n",
        "plt.xlabel(\"Classes\")\n",
        "plt.ylabel(\"F1-Score\")\n",
        "plt.title(\"Performance par classe\")\n",
        "plt.ylim(0, 1)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "gbPMcwBgbfIQ",
        "outputId": "cd2f6ca4-c157-44ec-dd97-8a952197411d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'X_sequences.npy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-81dc49a39298>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Charger les données\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X_sequences.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_labels.npy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    425\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'X_sequences.npy'"
          ]
        }
      ]
    }
  ]
}